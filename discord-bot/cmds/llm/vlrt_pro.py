import discord
from discord import app_commands
from discord.ext import commands
import traceback

from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

class VLRTPro(commands.Cog):
    def __init__(self, bot: commands.Bot):
        self.bot = bot
        print("ğŸ”„ è¼‰å…¥ RAG è³‡æ–™åº«...")

        embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        db = FAISS.load_local(
            "/discord-bot/cmds/llm/database/vlrt_pro_settings",
            embedding,
            allow_dangerous_deserialization=True
        )
        retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 5})

        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template="""
ä½ æ˜¯å°ˆæ¥­çš„é›»å­ç«¶æŠ€éšŠä¼è³‡æ–™åŠ©ç†ï¼Œæ ¹æ“šè³‡æ–™åº«ä¸­çš„è³‡è¨Šå›ç­”å•é¡Œã€‚
è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”æ‰€æœ‰å•é¡Œï¼Œä¸¦ç†è§£å•é¡Œä¸­æ‰€æœ‰ç”¨è©ç‚ºç¹é«”èªå¢ƒã€‚

è«‹åš´æ ¼éµå®ˆä»¥ä¸‹è¦æ±‚ï¼š
1. å›ç­”è¦ç°¡æ½”æ˜ç¢ºï¼Œè‹¥è³‡æ–™åº«ä¸­æ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ï¼Œè«‹ç›´æ¥å›è¦†ã€Œè³‡æ–™åº«ä¸­ç„¡ç›¸é—œè³‡è¨Šã€ã€‚
2. è‹¥å•é¡Œæ¶‰åŠéšŠä¼æˆå“¡ï¼Œè«‹åªåˆ—å‡ºè©²éšŠä¼æ‰€æœ‰é¸æ‰‹åå­—ï¼Œä¸å¤šé¤˜è§£é‡‹ã€‚
3. è‹¥å•é¡Œæ¶‰åŠæŸä½é¸æ‰‹çš„è¨­å‚™é…å‚™ï¼Œè«‹ç”¨æ¢åˆ—æ–¹å¼åˆ—å‡ºé—œéµé…å‚™è³‡è¨Šã€‚

ç”¨æˆ¶å•ï¼š{question}

ä»¥ä¸‹æ˜¯ä¾†è‡ªè³‡æ–™åº«çš„å…§å®¹ï¼š
{context}

ç­”è¦†ï¼š
"""
        )

        llm = OllamaLLM(model="llama3:8b")
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": prompt}
        )

    @app_commands.command(name="vlrt_pro", description="ä½¿ç”¨ Ollama3 æŸ¥è©¢ Valorant Pro Settings")
    @app_commands.describe(prompt="è«‹è¼¸å…¥ä½ æƒ³è©¢å•çš„å•é¡Œ")
    async def vlrt_pro(self, interaction: discord.Interaction, prompt: str):
        await interaction.response.defer(thinking=True)
        try:
            result = self.qa_chain.invoke({"query": prompt})
            output = result.get("result") or result.get("answer") or str(result)
        except Exception as e:
            print(traceback.format_exc())
            output = f"âš ï¸ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"

        # æ ¼å¼åŒ–è¨Šæ¯ï¼ŒåŒ…å«ä½¿ç”¨è€…è¼¸å…¥å’Œ AI è¼¸å‡º
        formatted_response = (
            f"**ä½ **ï¼š\n```\n{prompt}\n```"
            f"**ğŸ«§**ï¼š\n```\n{output}\n```"
        )

        await interaction.followup.send(formatted_response)

# Bot setup
async def setup(bot: commands.Bot):
    await bot.add_cog(VLRTPro(bot))